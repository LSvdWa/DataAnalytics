{
 "cells": [
  {
   "cell_type": "code",
   "id": "9f9cc532a184920b",
   "metadata": {},
   "source": [
    "#from numpy.core.numeric import infty\n",
    "!pip install numpy\n",
    "!pip install pandas\n",
    "!pip install plotly\n",
    "!pip install scikit-learn # non-depreceated sklearn"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2c742fe5da58a325",
   "metadata": {},
   "source": [
    "TASK 1"
   ]
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import pandas as pd\n",
    "# read the csv and make it a pandas.DataFrame\n",
    "frame = pd.DataFrame(pd.read_csv(\"./blood_transfusion.csv\"))\n",
    "frame.describe(include='all') # 5 columns, 4 numerical, class is nominal (1 or 0)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8112ffb8932f23c8",
   "metadata": {},
   "source": [
    "import plotly.express as px\n",
    "\n",
    "\"\"\"fig = px.histogram(frame.loc[frame['class'] == 1].Margin, x=\"Margin\", title=\"Histogram of Margin with Severity = 1\")\n",
    "fig.show()\"\"\"\n",
    "\n",
    "#2.3\n",
    "# A pie-chart showing how many cases of each class exist\n",
    "fig = px.pie(frame, values='months_since_last_donation', names='class', title='% of classes')\n",
    "fig.show()\n",
    "# Duidelijk verschil tussen het percentage 0 en 1 wanneer mensen een kortere periode hebben sinds laatste donatie\n",
    "fig = px.histogram(frame, x='months_since_last_donation',  color='class', title='')\n",
    "fig.show()\n",
    "# percentage redelijk gelijk over alle data\n",
    "fig = px.histogram(frame, x='months_since_first_donation',  color='class', title='')\n",
    "fig.show()\n",
    "# Meer donations is gelijk aan hoger percentage class 1\n",
    "fig = px.histogram(frame, x='total_number_of_donations',  color='class', title='')\n",
    "fig = px.scatter(frame, x='months_since_last_donation', y=\"total_number_of_donations\", color='class', title='')\n",
    "fig.show()\n",
    "fig = px.scatter(frame, x='months_since_last_donation', y=\"total_number_of_donations\", color='class', title='')\n",
    "fig.show()\n",
    "fig = px.histogram(frame, x='total_blood_donated',  color='class', title='')\n",
    "fig.show()\n",
    "fig = px.scatter(frame, x='months_since_last_donation', y=\"months_since_first_donation\", color='class', title='')\n",
    "fig.show()\n",
    "\n",
    "# total blood donated is linear with number of donations: dus wss niet nodig om beide te hebben in een vergelijking\n",
    "fig = px.scatter(frame, x='total_blood_donated', y=\"total_number_of_donations\", color='class', title='')\n",
    "fig.show()\n",
    "fig = px.scatter_3d(frame, x=\"months_since_last_donation\", y=\"total_blood_donated\", z='total_number_of_donations', color=\"class\", title=\"3d scatter\")\n",
    "fig.show()\n",
    "\n",
    "# Months since last donation is belangrijkst, spreid het meeste data, gevolgd door total blood donated. Miss een importance checker hier\n",
    "fig = px.scatter_3d(frame, x=\"months_since_last_donation\", y=\"total_blood_donated\", z='months_since_first_donation', color=\"class\", title=\"3d scatter\")\n",
    "fig.show()\n",
    "\n",
    "fig = px.scatter(frame, x='months_since_last_donation', y='months_since_first_donation', color='class', title=\"scatter\")\n",
    "fig.show()\n",
    "\n",
    "fig = px.scatter(frame, x='months_since_first_donation', y='months_since_last_donation', color='class', title=\"scatter\")\n",
    "fig.show()\n",
    "fig = px.histogram(frame, x='months_since_last_donation', color='class', title='')\n",
    "fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f3ad8613b85917f6",
   "metadata": {},
   "source": [
    "TASK 2"
   ]
  },
  {
   "cell_type": "code",
   "id": "7a917225e85e418c",
   "metadata": {},
   "source": [
    "preprocessed = frame.copy() \n",
    "preprocessed = (preprocessed-preprocessed.min())/(preprocessed.max()-preprocessed.min()) # Normalization\n",
    "# It also normalizes the class, but because class is either 1 or 0, the formula does not change the class \n",
    "preprocessed.describe()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "41518569b23bfe5f",
   "metadata": {},
   "source": [
    "TASK 3"
   ]
  },
  {
   "cell_type": "code",
   "id": "5efca73ed556e355",
   "metadata": {},
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train_s, X_test_s, Y_train_s, Y_test_s = train_test_split(\n",
    "    preprocessed.drop('class', axis=1), preprocessed['class'], train_size=0.33, test_size=0.33, random_state=1) # small test_size\n",
    "X_train_l, X_test_l, Y_train_l, Y_test_l = train_test_split(\n",
    "    preprocessed.drop('class', axis=1), preprocessed['class'], train_size=0.33, test_size=0.66, random_state=1) # large test_size\n",
    "# random state for reproducibility"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2b167550ba199f9c",
   "metadata": {},
   "source": [
    "\"\"\"\"NIET MET DE OPDRACHT TE MAKEN, FEATURE IMPORTANCE VOOR DE PRESENTATIE\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "names = (list(X_train_s))\n",
    "feature_names = [f\"feature {names[i]}\" for i in range(X_train_s.shape[1])]\n",
    "forest = RandomForestClassifier(random_state=0)\n",
    "forest.fit(X_train_s, Y_train_s)\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "start_time = time.time()\n",
    "importances = forest.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in forest.estimators_], axis=0)\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(f\"Elapsed time to compute the importances: {elapsed_time:.3f} seconds\")\n",
    "forest_importances = pd.Series(importances, index=feature_names)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "forest_importances.plot.bar(yerr=std, ax=ax)\n",
    "ax.set_title(\"Feature importances using MDI\")\n",
    "ax.set_ylabel(\"Mean decrease in impurity\")\n",
    "fig.tight_layout()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "25d2e5f6c9684691",
   "metadata": {},
   "source": [
    "TASK 4 & 5"
   ]
  },
  {
   "cell_type": "code",
   "id": "5fd32bd75029c3fc",
   "metadata": {},
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import fbeta_score\n",
    "\n",
    "#5.1 Confusion matrix\n",
    "def confusionmatrix(predicted_y, real_y): # it works, im confused :) \\j\n",
    "    \"\"\"\n",
    "    This function calculates and prints the confusion matrix.\n",
    "    :param predicted_y: list: predicted classes for the test data \n",
    "    :param real_y: list: real classes of the test data\n",
    "    :return: prints the confusion matrix, returns nothing\n",
    "    \"\"\"\n",
    "    tp, fp, tn, fn = 0, 0, 0, 0\n",
    "    for index,_ in enumerate(predicted_y):\n",
    "        if predicted_y[index][0] == 1: \n",
    "            if predicted_y[index] == real_y[index]: \n",
    "                tp += 1\n",
    "            else:\n",
    "                fp += 1\n",
    "        elif predicted_y[index][0] == 0:\n",
    "            if predicted_y[index] == real_y[index]: \n",
    "                tn += 1\n",
    "            else:\n",
    "                fn += 1\n",
    "    print(f\"\\t P \\t\\t N \\n T \\t {tp} \\t\\t {tn} \\n F \\t {fp} \\t\\t {fn} \\n\")\n",
    "\n",
    "def accuracy_checker(X_test, y_test, pred, mode=\"basic\"):\n",
    "    \"\"\"\n",
    "    This function checks the accuracy of the predicted classes against the real classes.\n",
    "    :param X_test: pandas.DataFrame: the test data to be classified\n",
    "    :param y_test: pandas.Series: the real classes of the test data\n",
    "    :param pred: predictor class: the method used to classify the test data\n",
    "    :param mode: string: way to evaluate the results\n",
    "    :return: prints the results, returns nothing\n",
    "    \"\"\"\n",
    "    num, total = 0, 0\n",
    "    predicted, true = [], []\n",
    "    for index, row in X_test.iterrows():\n",
    "        p = pred.predict(X_test.loc[[index]])\n",
    "        q = list(y_test.loc[[index]])\n",
    "        if p == q:\n",
    "            num += 1\n",
    "        total += 1\n",
    "        predicted.append(p)\n",
    "        true.append(q)\n",
    "    if mode == \"basic\":\n",
    "        print(f\"Accuracy: {num/total*100:.2f}%\")\n",
    "    elif mode == \"classification_report\":\n",
    "        print(classification_report(true, predicted))\n",
    "    elif mode == \"confusion_matrix\":\n",
    "        confusionmatrix(predicted, true)\n",
    "    elif mode == \"fbeta\":\n",
    "        print(\"average=macro:\", fbeta_score(true, predicted, average='macro', beta=0.5))\n",
    "        print(\"average=weighted:\", fbeta_score(true, predicted, average='weighted', beta=0.5))\n",
    "        print(\"average=None:\", fbeta_score(true, predicted, average=None, beta=0.5))\n",
    "    else:\n",
    "        return num/total*100\n",
    "\n",
    "# 4.1 KNN\n",
    "class KNN:    \n",
    "    def __init__(self, k=5):\n",
    "        \"\"\"\n",
    "        initializes an instance of the KNN class\n",
    "        :param k: int: number of neighbors\n",
    "        \"\"\"\n",
    "        self.k = k\n",
    "        self.X_train = None\n",
    "        self.y_train = None\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        fits the KNN class\n",
    "        :param X: pandas.DataFrame: the train data\n",
    "        :param y: pandas.Series: the classes of the train data\n",
    "        :return: a fitted instance of the KNN class\n",
    "        \"\"\"\n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "        return self\n",
    "    \n",
    "    def euclidean_distance(self, a, b):\n",
    "        \"\"\"\n",
    "        This function calculates the euclidean distance between two datapoints.\n",
    "        :param a: list: the first datapoint\n",
    "        :param b: list: the second datapoint\n",
    "        :return: float: the euclidean distance between the datapoints\n",
    "        \"\"\"\n",
    "        distance = 0\n",
    "        for index, _ in enumerate(a):\n",
    "            distance += (a[index] - b[index]) ** 2\n",
    "        distance = distance**0.5\n",
    "        return distance    \n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        \"\"\"\n",
    "        predicts the class of the test data\n",
    "        :param X_test: pandas.DataFrame: the test data\n",
    "        :return: list: the predicted classes\n",
    "        \"\"\"\n",
    "        test_y = list([0 for _ in range(X_test.shape[0])])\n",
    "        for index, (_,test_row) in enumerate(X_test.iterrows()):\n",
    "            distances = []\n",
    "            for _, train_row in self.X_train.iterrows():\n",
    "                test_values = list(test_row)  \n",
    "                train_values = list(train_row)\n",
    "                distance = self.euclidean_distance(test_values, train_values)\n",
    "                distances.append(distance)\n",
    "            nearest_neighbours = sorted(range(len(distances)), key=lambda i: distances[i])[:self.k]\n",
    "            neighbor_labels = self.y_train.iloc[nearest_neighbours]\n",
    "            predicted_label = round(neighbor_labels.mean())\n",
    "            test_y[index] = predicted_label\n",
    "        return test_y\n",
    "\n",
    "# from accuracy it appears that k=5 is best for small set and k=[2,4,9] are all best for large set, for 1<=k<=9\n",
    "knn_s = KNN(5)\n",
    "knn_s.fit(X_train_s, Y_train_s)\n",
    "knn_s_acc = accuracy_checker(X_test_s, Y_test_s, knn_s, \"return\")\n",
    "knn_l = KNN(5)\n",
    "knn_l.fit(X_train_l, Y_train_l)\n",
    "knn_l_acc = accuracy_checker(X_test_l, Y_test_l, knn_l, \"return\")\n",
    "\n",
    "# 4.2 nbc\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "nbc_s = GaussianNB()\n",
    "nbc_s.fit(X_train_s, Y_train_s)\n",
    "nbc_s_acc = accuracy_checker(X_test_s, Y_test_s, nbc_s, \"return\")\n",
    "nbc_l = GaussianNB()\n",
    "nbc_l.fit(X_train_l, Y_train_l)\n",
    "nbc_l_acc = accuracy_checker(X_test_l, Y_test_l, nbc_l, \"return\")\n",
    "\n",
    "# 4.3 svc\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "svc_s = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
    "svc_s.fit(X_train_s, Y_train_s)\n",
    "svc_s_acc = accuracy_checker(X_test_s, Y_test_s, svc_s, \"return\")\n",
    "svc_l = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
    "svc_l.fit(X_train_l, Y_train_l)\n",
    "svc_l_acc = accuracy_checker(X_test_l, Y_test_l, svc_l, \"return\")\n",
    "\n",
    "# 4.4 mlp\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp_s = MLPClassifier(random_state=1, max_iter=400).fit(X_train_s, Y_train_s)\n",
    "mlp_s_acc = accuracy_checker(X_test_s, Y_test_s, mlp_s, \"return\")\n",
    "mlp_l = MLPClassifier(random_state=1, max_iter=400).fit(X_train_l, Y_train_l)\n",
    "mlp_l_acc = accuracy_checker(X_test_l, Y_test_l, mlp_l, \"return\")\n",
    "\n",
    "classifiers_list = [\"knn_s\", \"knn_l\", \"nbc_s\", \"nbc_l\", \"svc_s\", \"svc_l\", \"mlp_s\", \"mlp_l\"]\n",
    "accuracy_list = [knn_s_acc, knn_l_acc, nbc_s_acc, nbc_l_acc, svc_s_acc, svc_l_acc, mlp_s_acc, mlp_l_acc]\n",
    "fig = px.histogram(x=classifiers_list, y=accuracy_list, barmode='group', height=500, title='Classifiers and accuracy', text_auto=True)\n",
    "fig.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8aadb811fdda4bdb",
   "metadata": {},
   "source": [
    "preds = [knn_s, knn_l, nbc_s, nbc_l, svc_s, svc_l, mlp_s, mlp_l]\n",
    "modes = [\"confusion_matrix\", \"classification_report\", \"fbeta\"]\n",
    "for mode in modes:\n",
    "    for pred in preds:\n",
    "        pred_string = \"\"\n",
    "        if pred == svc_s: pred_string = \"svc_s\"\n",
    "        elif pred == svc_l: pred_string = \"svc_l\"\n",
    "        elif pred == mlp_s: pred_string = \"mlp_s\"\n",
    "        elif pred == mlp_l: pred_string = \"mlp_l\"\n",
    "        elif pred == nbc_s: pred_string = \"nbc_s\"\n",
    "        elif pred == nbc_l: pred_string = \"nbc_l\"\n",
    "        elif pred == knn_s: pred_string = \"knn_s\"\n",
    "        elif pred == knn_l: pred_string = \"knn_l\"\n",
    "        if pred_string[4] == \"s\":\n",
    "            print(f\"{mode} for {pred_string}\")\n",
    "            accuracy_checker(X_test_s, Y_test_s, pred, mode=mode)    \n",
    "        elif pred_string[4] == \"l\":\n",
    "            print(f\"\\n{mode} for {pred_string}\")\n",
    "            accuracy_checker(X_test_l, Y_test_l, pred, mode=mode)\n",
    "        print(\"-----------------------------------------\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "656d5854",
   "metadata": {},
   "source": [
    "TASK 6"
   ]
  },
  {
   "cell_type": "code",
   "id": "ee48293a89ef8b5f",
   "metadata": {},
   "source": [
    "\n",
    "from sklearn.model_selection import KFold\n",
    "import random\n",
    "X = preprocessed.copy()\n",
    "\n",
    "random_tester = []\n",
    "for b in range(0,8):\n",
    "    kf = KFold(n_splits=3, shuffle=True, random_state=random.randint(0,10000))\n",
    "    kf.get_n_splits()\n",
    "\n",
    "    print(kf)\n",
    "    folds = []\n",
    "    KFold(n_splits=3, random_state=None, shuffle=False)\n",
    "    for (train_index, test_index) in (kf.split(X)):\n",
    "        folds.append((train_index, test_index))\n",
    "\n",
    "    accuracy = []\n",
    "    parameters = [2,4,5,9]\n",
    "\n",
    "    for m, (i,j) in enumerate(folds):\n",
    "        temptrainx = X.iloc[list(i)].drop('class', axis=1)\n",
    "        temptrainy = X.iloc[list(i)]['class']\n",
    "        temptestx = X.iloc[list(j)].drop('class', axis=1)\n",
    "        temptesty = X.iloc[list(j)]['class']\n",
    "\n",
    "        knn_s = KNN(parameters[m])\n",
    "        knn_s.fit(temptrainx, temptrainy)\n",
    "        accuracy.append(accuracy_checker(temptestx, temptesty, knn_s,mode=\"basicRET\"))\n",
    "\n",
    "    print(max(accuracy), f\"Best number of neighbors is: {parameters[accuracy.index(max(accuracy))]}\")\n",
    "    random_tester.append(parameters[accuracy.index(max(accuracy))])\n",
    "print(random_tester)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# visualisation\n",
    "\n",
    "kf = KFold(n_splits=3, shuffle=False)\n",
    "kf.get_n_splits()\n",
    "\n",
    "print(kf)\n",
    "folds = []\n",
    "KFold(n_splits=3, random_state=None, shuffle=False)\n",
    "for (train_index, test_index) in (kf.split(X)):\n",
    "    folds.append((train_index, test_index))\n",
    "randomint = max(set(random_tester), key=random_tester.count)\n",
    "parameters =  [randomint for i in range(1,4)]\n",
    "accuracy = []\n",
    "\n",
    "for m, (i,j) in enumerate(folds):\n",
    "    temptrainx = X.iloc[list(i)].drop('class', axis=1)\n",
    "    temptrainy = X.iloc[list(i)]['class']\n",
    "    temptestx = X.iloc[list(j)].drop('class', axis=1)\n",
    "    temptesty = X.iloc[list(j)]['class']\n",
    "\n",
    "    knn_s = KNN(parameters[m])\n",
    "    knn_s.fit(temptrainx, temptrainy)\n",
    "    accuracy.append(accuracy_checker(temptestx, temptesty, knn_s, mode=\"return\"))\n",
    "print(max(accuracy), f\"Best number of neighbors is: {parameters[accuracy.index(max(accuracy))]}\")\n",
    "\n",
    "fig = px.bar(x=[x for x in range(1,len(folds)+1)], y= accuracy, title='Accuracy per fold', text_auto=True)\n",
    "fig.update_layout(yaxis_title=\"accuracy\", xaxis_title=\"\")\n",
    "fig.show()"
   ],
   "id": "ba0fdf75c909924f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "d7533a0943f1139a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "eec8cb73ceb99d78",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
