{
 "cells": [
  {
   "cell_type": "code",
   "id": "6198313a0fd8ef21",
   "metadata": {},
   "source": [
    "#from numpy.core.numeric import infty\n",
    "!pip install numpy\n",
    "!pip install pandas\n",
    "!pip install plotly\n",
    "\n",
    "!pip install scikit-learn # non-depreceated sklearn"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c04bec9720cf8181",
   "metadata": {},
   "source": [
    "Task 1"
   ]
  },
  {
   "cell_type": "code",
   "id": "569340ce255da6ee",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import pandas as pd\n",
    "# read the csv and make it a pandas.DataFrame\n",
    "frame = pd.DataFrame(pd.read_csv(\"./online_shoppers_intention.csv\"))\n",
    "frame.describe(include='all') # 18 columns, including 2 categorical: Month & VisitorType, and two boolean: Weekend & Revenue"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "619404bddacffcd3",
   "metadata": {},
   "source": [
    "#visualisation\n",
    "# split the Browser 13 users from the rest\n",
    "frame13 = frame.loc[frame.Browser == 13]\n",
    "frameOther = frame.loc[frame.Browser != 13]\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "# make a small dataframe with normalized data for plotting\n",
    "def create_distribution(frameColumn):\n",
    "    return pd.DataFrame({\n",
    "        'Browser 13': frame13[frameColumn].value_counts(normalize=True),\n",
    "        'Other browsers': frameOther[frameColumn].value_counts(normalize=True) \n",
    "    }).reset_index().rename(columns={'index': col}) \n",
    "# plot all these columns\n",
    "for col in ['VisitorType', 'Region', 'Month']:\n",
    "    distribution = create_distribution(col)\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Bar(x=distribution[col], y=distribution['Browser 13'], name='Browser 13'))\n",
    "    fig.add_trace(go.Bar(x=distribution[col], y=distribution['Other browsers'], name='Other browsers'))\n",
    "    fig.update_layout(barmode='group', title=f'{col} Distribution')\n",
    "    fig.update_yaxes(title='Count')\n",
    "    fig.update_xaxes(title=f'{col}')\n",
    "    fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e1d6f54fce662810",
   "metadata": {},
   "source": [
    "Task 2"
   ]
  },
  {
   "cell_type": "code",
   "id": "b3be118550c59a04",
   "metadata": {},
   "source": [
    "#Prepositioning op basis van algoritmes in task 3\n",
    "# Preprocess by changing all string types to an integer\n",
    "def to_num(x):\n",
    "    data = {\n",
    "        \"Jan\": 1, \"Feb\": 2, \"Mar\": 3, \"Apr\": 4,\n",
    "        \"May\": 5, \"June\": 6, \"Jul\": 7, \"Aug\": 8,\n",
    "        \"Sep\": 9, \"Oct\": 10, \"Nov\": 11, \"Dec\": 12, \n",
    "        \"Returning_Visitor\": 1, \"Old_Visitor\": 2, \"New_Visitor\": 3, \"Other\": 4, \n",
    "        True: 1, False: 0\n",
    "    }\n",
    "    if x not in data:\n",
    "        raise ValueError(f\"Invalid month string: {x}\")\n",
    "    return data[x]\n",
    "\n",
    "preprocessed = (frame.copy().dropna()) # dropna() is added here for a case where the dataset would include missing values, but does nothing with the original dataset as it does not contain any\n",
    "# converts strings and booleans to numbers\n",
    "columns = ['Month', 'VisitorType', 'Weekend', 'Revenue']\n",
    "for column in columns:\n",
    "    addlist = []\n",
    "    for i, j in enumerate(preprocessed[column]):\n",
    "        addlist.append(to_num(j))\n",
    "    preprocessed[column] = addlist\n",
    "\n",
    "preprocessed = (preprocessed-preprocessed.min())/preprocessed.max()-preprocessed.min() # Normalization\n",
    "print(preprocessed)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ca05fe789cf17ab3",
   "metadata": {},
   "source": [
    "Task 3"
   ]
  },
  {
   "cell_type": "code",
   "id": "a64c29fcca7de65f",
   "metadata": {},
   "source": [
    "# affinity propagation clustering - based on documentation - VERY SLOW FOR WHOLE DATASET\n",
    "from sklearn.cluster import AffinityPropagation, DBSCAN, Birch\n",
    "import plotly.express as px\n",
    "Data = preprocessed.copy()\n",
    "# By sampling it goes faster\n",
    "Sample = Data.sample(n=1000)\n",
    "clustering = AffinityPropagation(random_state=2).fit(Sample)\n",
    "AffinityPropagation(random_state=2)\n",
    "\n",
    "labels = clustering.labels_\n",
    "cluster_centers = clustering.cluster_centers_\n",
    "n_clusters = len(cluster_centers)\n",
    "\n",
    "print(\"Number of clusters:\", n_clusters)\n",
    "print(\"Cluster centers:\\n\", cluster_centers)\n",
    "\n",
    "\n",
    "# DBSCAN clustering\n",
    "# It works, but a lot of datapoints get labeled -1, meaning there is a lot of noise\n",
    "# It has less clusters compared to the other methods\n",
    "clustering2 = DBSCAN().fit(Data)\n",
    "labels2 = clustering2.labels_\n",
    "print(\"Labels DBSCAN\", labels2[:50])   \n",
    "\n",
    "# Birch clustering\n",
    "clustering3 = Birch(n_clusters=None).fit(Data)\n",
    "labels3 = clustering3.labels_\n",
    "print(len(labels3), len(Data))\n",
    "print(\"Labels Birch\", labels3[:50])   \n",
    "aantal_clusters = len(set(labels3))\n",
    "print(\"Aantal clusters Birch\", aantal_clusters)\n",
    "\n",
    "fig = px.scatter(Sample, labels)\n",
    "fig.show()\n",
    "fig = px.scatter(Data, labels2)\n",
    "fig.show()\n",
    "fig = px.scatter(Data, labels3)\n",
    "fig.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#TODO visualisaties van task 3\n",
    "# in apart codeblock "
   ],
   "id": "7505cad42ca4aa05",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2d8d2baa9a81b136",
   "metadata": {},
   "source": [
    "Task 4"
   ]
  },
  {
   "cell_type": "code",
   "id": "878a2061917b83d8",
   "metadata": {},
   "source": [
    "# manual Silhouette score, is zo volgensmij goed. Moet alleen zoeken hoe ik deze waardes kan pakken uit de labels\n",
    "import numpy as np\n",
    "def Silhouette_score(point, labelSELF, labels, Data):\n",
    "    clusterSELF = Data[labels == labelSELF].to_numpy()\n",
    "    clusters = Data[labels != labelSELF].to_numpy()\n",
    "    afstand = 0\n",
    "\n",
    "    for points in clusterSELF:\n",
    "        if set(points) == set(point):\n",
    "            continue\n",
    "        afstand += np.linalg.norm(point-points)\n",
    "    a = afstand/len(clusterSELF)\n",
    "\n",
    "    lowest = 999999999\n",
    "    for clust in clusters:\n",
    "        afstand_nearest = 0\n",
    "        for points in clust:\n",
    "            afstand_nearest += np.linalg.norm(point-points)\n",
    "        dis = afstand_nearest/len(clust)\n",
    "        if dis < lowest:\n",
    "            lowest = dis\n",
    "    b = lowest\n",
    "    return (b-a)/(max(b,a))\n",
    "\n",
    "def Silhouette_scores(x, labels):\n",
    "    n = len(x)\n",
    "    sil_values = []\n",
    "    for i in range(n):\n",
    "        sil_values.append(Silhouette_score(point=x.iloc[i].to_numpy(), labelSELF = labels[i], labels = labels, Data= x))\n",
    "    print(\"Silhouette score:\", sil_values)\n",
    "\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "print(\"Silhouette scores:\")\n",
    "print(f\"Affinity propagation: {Silhouette_scores(Sample, labels)}\")\n",
    "print(f\"DBSCAN: {Silhouette_scores(Data[:1000], labels2[:1000])}\")\n",
    "print(f\"Birch Clustering: {Silhouette_scores(Data[:1000], labels3[:1000])}\")\n",
    "print()\n",
    "\n",
    "# Davies Buildin Score\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "print(\"Davies Bouldin score:\")\n",
    "print(f\"Affinity propogation: {davies_bouldin_score(Sample, labels)}\")\n",
    "print(f\"DBSCAN: {davies_bouldin_score(Data, labels2)}\")\n",
    "print(f\"Birch Clustering: {davies_bouldin_score(Data, labels3)}\")\n",
    "print()\n",
    "# calinski-harabasz index\n",
    "from sklearn.metrics import calinski_harabasz_score\n",
    "print(\"Calinski Harabasz score:\")\n",
    "print(f\"Affinity propogation: {calinski_harabasz_score(Sample, labels)}\")\n",
    "print(f\"DBSCAN: {calinski_harabasz_score(Data, labels2)}\")\n",
    "print(f\"Birch Clustering: {calinski_harabasz_score(Data, labels3)}\")\n",
    "#TODO visualize"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "12deb65d3f7f63eb",
   "metadata": {},
   "source": [
    "Task 5 ALL MANUAL\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "2a95a246f6bf2fa2",
   "metadata": {},
   "source": [
    "# dit is de basics, maar idk wat de dimensies van a en b horen te zijn\n",
    "# Euclidean distance\n",
    "def our_euclidean_distance(a, b):\n",
    "    distance = 0\n",
    "    for index, _ in enumerate(a):\n",
    "        distance += (a[index] - b[index]) ** 2\n",
    "    distance = distance**0.5\n",
    "    return distance\n",
    "# Manhattan distance\n",
    "def our_manhattan_distance(a, b):\n",
    "    distance = 0\n",
    "    for index, _ in enumerate(a):\n",
    "        distance += ((a[index] - b[index]) ** 2) ** 0.5\n",
    "    return distance    \n",
    "# Cosine similarity\n",
    "def our_cosine_similarity(a, b):\n",
    "    numerator = 0\n",
    "    denominator_a = 0\n",
    "    denominator_b = 0\n",
    "    for index, _ in enumerate(a):\n",
    "        numerator += a[index] * b[index]\n",
    "        denominator_a += a[index] ** 2\n",
    "        denominator_b += b[index] ** 2\n",
    "    distance = numerator / (denominator_a ** 0.5 * denominator_b ** 0.5)\n",
    "    return distance\n",
    "# Distances effect on dbscan\n",
    "clustering_c1 = DBSCAN(metric=our_euclidean_distance).fit(Data[:1000])\n",
    "label_c1 = clustering_c1.labels_\n",
    "print(\"Labels DBSCAN\", label_c1)   \n",
    "\n",
    "clustering_c2 = DBSCAN(metric=our_manhattan_distance).fit(Data[:1000])\n",
    "label_c2 = clustering_c2.labels_\n",
    "print(\"Labels DBSCAN\", label_c2)   \n",
    "\n",
    "clustering_c3 = DBSCAN(metric=our_cosine_similarity).fit(Data[:1000])\n",
    "label_c3 = clustering_c3.labels_\n",
    "print(\"Labels DBSCAN\", label_c3)   \n",
    "\n",
    "import plotly.express as px\n",
    "fig = px.scatter(Data[:1000], label_c1)\n",
    "fig.show()\n",
    "fig = px.scatter(Data[:1000], label_c2)\n",
    "fig.show()\n",
    "#TODO labels zijn allemaal -1\n",
    "fig = px.scatter(Data[:1000], label_c3)\n",
    "fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "f6e23856040adca3",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
